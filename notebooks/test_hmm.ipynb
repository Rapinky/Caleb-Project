{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HMM POS Tagger: Final Evaluation & Analysis\n",
                "This notebook presents the final results of the Hidden Markov Model POS Tagger, including comparisons against a baseline and detailed error analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.append(os.path.abspath('../src'))\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "from utils import load_pos_data, get_vocab_and_tags, download_nltk_data\n",
                "from train_hmm import train_hmm\n",
                "from evaluate import viterbi, evaluate_models, train_mft_baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 1. Setup Data\n",
                "download_nltk_data()\n",
                "train_data, test_data = load_pos_data()\n",
                "# Using min_freq=2 to make UNK emission probabilities more robust\n",
                "vocab, tags = get_vocab_and_tags(train_data, min_freq=2)\n",
                "\n",
                "print(f\"Training sentences: {len(train_data)}\")\n",
                "print(f\"Vocabulary size (min_freq=2): {len(vocab)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 2. Train Models\n",
                "hmm_model = train_hmm(train_data, vocab, tags)\n",
                "mft_model = train_mft_baseline(train_data, tags)\n",
                "print(\"Training complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 3. Run Quantitative Evaluation\n",
                "results = evaluate_models(test_data[:300], hmm_model, mft_model)\n",
                "\n",
                "metrics = {\n",
                "    \"Model\": [\"HMM (Viterbi)\", \"Baseline (MFT)\"],\n",
                "    \"Accuracy\": [results['hmm_accuracy'], results['mft_accuracy']]\n",
                "}\n",
                "display(pd.DataFrame(metrics))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 4. Error Analysis: Confusion Matrix\n",
                "plt.figure(figsize=(10, 8))\n",
                "cm = results['confusion_matrix']\n",
                "# Normalize confusion matrix to show percentages\n",
                "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
                "            xticklabels=results['tags'], yticklabels=results['tags'])\n",
                "plt.title(\"Normalized HMM Confusion Matrix (True vs Predicted)\")\n",
                "plt.ylabel(\"True Tag\")\n",
                "plt.xlabel(\"Predicted Tag\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# 5. Top 10 Most Common Errors\n",
                "errors = []\n",
                "for i in range(len(results['tags'])):\n",
                "    for j in range(len(results['tags'])):\n",
                "        if i != j:\n",
                "            errors.append((results['tags'][i], results['tags'][j], cm[i, j]))\n",
                "\n",
                "errors = sorted(errors, key=lambda x: x[2], reverse=True)\n",
                "print(\"Top 10 Tagging Errors (True -> Predicted):\")\n",
                "for true, pred, count in errors[:10]:\n",
                "    print(f\"{true:5} -> {pred:5} : {count} occurrences\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Interactive Tagging\n",
                "Run the cell below to tag your own sentences!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "user_input = \"The quick brown fox jumps over the lazy dog\"\n",
                "# user_input = input(\"Enter a sentence to tag: \") # Uncomment in traditional Jupyter environments\n",
                "\n",
                "words = user_input.split()\n",
                "preds = viterbi(words, hmm_model)\n",
                "\n",
                "print(f\"Sentence: {user_input}\")\n",
                "print(\"-\"*30)\n",
                "for w, t in zip(words, preds):\n",
                "    print(f\"{w:15} | {t}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}